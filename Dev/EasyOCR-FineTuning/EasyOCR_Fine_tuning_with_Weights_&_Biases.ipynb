{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"40%\" alt=\"Weights & Biases Logo\" />\n",
        "\n",
        "# Introduction\n",
        "\n",
        "In this Notebook we'll take one of the common OCR libraries, EasyOCR, an fine-tune the model that it uses for prediction. You can then take that trained model and then call the model using the EasyOCR API to make predictions on images of text.\n",
        "\n",
        "## Training a custom model\n",
        "\n",
        "#### Using open-source data\n",
        "To train your EasyOCR model you can use your own data / generate your own dataset using a tool like [TextRecognitionDataGenerator](https://github.com/Belval/TextRecognitionDataGenerator).\n",
        "\n",
        "The default network in the EasyOCR library is 'None-VGG-BiLSTM-CTC'. In addition to a trained model file in the form of a `.pth` file you will need two files: a network architecture file and a model configuration file. You can see some sample files on this EasyOCR Model Hub page which offers English, Latin, Chinese, Japanese, Korean, Telugu, and Kannada under the Second Gen Models. Under the First Generation Models you can choose between Latin, Chinese (Simple), Chinese (Traditional), Japanese, Korean, Thai, Devanagari, Cyrillic, Arabic, Tamil, Bengali: https://jaided.ai/easyocr/modelhub/\n",
        "\n",
        "#### Via JaidedAI\n",
        "\n",
        "If you'd like to pay for a web-based service to fine-tune an EasyOCR model you can use JaidedAI's training service.\n",
        "\n",
        "\n",
        "# Steps taken for open-source training of your own recognition model\n",
        "\n",
        "We'll go with the free option. We've outlined the steps taken to fine-tune EasyOCR below after reading through and synthesizing the EasyOCR documentation:\n",
        "\n",
        "1. Generate a dataset of text images (using something like [TextRecognitionDataGenerator](https://github.com/Belval/TextRecognitionDataGenerator) or by BYOD - Bringing Your Own Data - if you already have a corpus of images of text and their labels.\n",
        "2. After you have your dataset you will want to train your model using the `deep-text-recognition-benchmark` library: https://github.com/clovaai/deep-text-recognition-benchmark This is a **PyTorch**-based library with a great deal of benchmark datasets for you to use: IMBD, ICDAR, etc. The network needs to be fully connected in order to predict flexible text length. The authors of the library used `None-VGG-BiLSTM-CTC` for their model architecture.\n",
        "3. Once you have a trained model (and the `.pth` file that the EasyOCR library produces) you will need **two** additional files: 1 file describing the network architecture and 1 file describing the model configuration. The **Custom Model** file on the EasyOCR Model Hub page contains an example of the two files: https://jaided.ai/easyocr/modelhub/\n",
        "\n",
        "# Which model should I fine-tune?\n",
        "\n",
        "Depending on what model you want to train / fine-tune you'll have a range of performance scores. Below is performance curve taken from the \"What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis\" paper at https://arxiv.org/abs/1904.01906.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/clovaai/deep-text-recognition-benchmark/master/figures/trade-off.png\" width=\"80%\" alt=\"Proposed combinations and performance curves\" />\n"
      ],
      "metadata": {
        "id": "kfoDlr7r617l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting started"
      ],
      "metadata": {
        "id": "oWuBpgHD83Gf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the EasyOCR fine-tuning repo, we know that in order to fine tune our model we follow the instructions here: https://github.com/clovaai/deep-text-recognition-benchmark#getting-started\n",
        "\n",
        "Basically `pip install` some libraries (most of which are already installed in Colab by default). Then we'll edit the `train.py` script to include `wandb` and write out the progress of our training to our Weights and Biases dashboard.\n"
      ],
      "metadata": {
        "id": "suzlpUkV8o-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -qqq lmdb pillow torchvision nltk natsort wandb gdown\n",
        "\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "6zrOD5pq8lm5",
        "outputId": "2ba81742-f457-4e1c-f5dc-ab2661cadabd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8 MB 14.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 145 kB 95.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 74.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ7u_jsq60R4",
        "outputId": "a2da89cc-f387-4551-d5b6-14689096e476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deep-text-recognition-benchmark'...\n",
            "remote: Enumerating objects: 495, done.\u001b[K\n",
            "remote: Total 495 (delta 0), reused 0 (delta 0), pack-reused 495\u001b[K\n",
            "Receiving objects: 100% (495/495), 3.07 MiB | 31.09 MiB/s, done.\n",
            "Resolving deltas: 100% (302/302), done.\n"
          ]
        }
      ],
      "source": [
        "# Download the repo that has the code that you can reference to fine-tune / train\n",
        "!git clone https://github.com/clovaai/deep-text-recognition-benchmark.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading Data\n",
        "\n",
        "Per the instructions, download the training data which is located in a [Dropbox folder](https://www.dropbox.com/sh/i39abvnefllx2si/AAAbAYRvxzRp3cIE5HzqUw3ra?dl=0). Note that the `data_lmdb_release.zip` file contains the training, validation **and** test datasets. It is, however, ~18GB in size, so it may take some time to download depending on your connection.\n",
        "\n",
        "```\n",
        "- data_lmdb_release.zip contains training, validation, and evaluation sets.\n",
        "\n",
        "- validation.zip contains only validation set.\n",
        "\n",
        "- evaluation.zip contains only evaluation set.\n",
        "\n",
        "- ST_spe.zip contains word images, which include special characters in SynthText (ST) dataset.\n",
        "check this issue https://github.com/clovaai/deep-text-recognition-benchmark/issues/7#issuecomment-511727025\n",
        "```"
      ],
      "metadata": {
        "id": "LJ_BbvkJ9akv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd deep-text-recognition-benchmark/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1pQgMgD8kd8",
        "outputId": "cbcd51a6-db9d-4cb1-c2d5-5f1feed919bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deep-text-recognition-benchmark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's easiest to work with the `deep-text-recognition-benchmark` tool if you simply download the LMDB dataset so that it is inside of the `deep-text-recognition-benchmark` directory.\n",
        "\n",
        "Having the dataset outside of that directory will require you to do a considerable amount of editing of training and validation scripts to allow the `deep-text-recognition-benchmark` to run properly and 'find' the datasets.\n",
        "\n",
        "Note that instead of `wget`-ing the dataset from the authors you could download a copy from Weights and Biases Artifacts here: https://wandb.ai/andrea0/deep-text-recognition-benchmark/artifacts/compressed-dataset/lmdb-dataset-zip/68b56d59f046d42ea5ce\n",
        "\n",
        "To download an artifact and make use of it simply:\n",
        "```python\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "# Pull down that dataset you logged in the last run\n",
        "artifact = run.use_artifact('lmdb-dataset-zip:latest')\n",
        "artifact_dir = artifact.download()\n",
        "\n",
        "# Save a model after training\n",
        "model = wandb.Artifact('my-model', type='model')\n",
        "model.add_file('my-model.txt')\n",
        "run.log_artifact(model)\n",
        "\n",
        "wandb.finish()\n",
        "```"
      ],
      "metadata": {
        "id": "Nz_Li-cV9rPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/sh/i39abvnefllx2si/AABX4yjNn2iLeKZh1OAwJUffa/data_lmdb_release.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_3BBf_o8tJn",
        "outputId": "906ab7a0-3f75-42cc-d367-1846c3921992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-01 01:51:43--  https://www.dropbox.com/sh/i39abvnefllx2si/AABX4yjNn2iLeKZh1OAwJUffa/data_lmdb_release.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6021:18::a27d:4112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /sh/raw/i39abvnefllx2si/AABX4yjNn2iLeKZh1OAwJUffa/data_lmdb_release.zip [following]\n",
            "--2022-06-01 01:51:43--  https://www.dropbox.com/sh/raw/i39abvnefllx2si/AABX4yjNn2iLeKZh1OAwJUffa/data_lmdb_release.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucb6bbbf7a5596319cbe64ade88b.dl.dropboxusercontent.com/cd/0/inline/BmWQA_IPII9LvMMHCnoAjs9mHsLxGaMziUSvGIrLSxDBT_Q132sj5nc-7EwPxbaOCc06Ogxriiwl3LzBQ8LXhx9GejGyHr306jqRmVG29SlAG-s7hKOP6eWEDPLC882VsEiGMzABOT7D-QbNFrFu_VUy9Oj8YBARQxL5o0vWOfUZMg/file# [following]\n",
            "--2022-06-01 01:51:44--  https://ucb6bbbf7a5596319cbe64ade88b.dl.dropboxusercontent.com/cd/0/inline/BmWQA_IPII9LvMMHCnoAjs9mHsLxGaMziUSvGIrLSxDBT_Q132sj5nc-7EwPxbaOCc06Ogxriiwl3LzBQ8LXhx9GejGyHr306jqRmVG29SlAG-s7hKOP6eWEDPLC882VsEiGMzABOT7D-QbNFrFu_VUy9Oj8YBARQxL5o0vWOfUZMg/file\n",
            "Resolving ucb6bbbf7a5596319cbe64ade88b.dl.dropboxusercontent.com (ucb6bbbf7a5596319cbe64ade88b.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6021:15::a27d:410f\n",
            "Connecting to ucb6bbbf7a5596319cbe64ade88b.dl.dropboxusercontent.com (ucb6bbbf7a5596319cbe64ade88b.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BmWp_5eOvU6UFOkYKVmeNqKVnQWFLQjVEkAHC0k_IDI7en86QoNDXO9ZccgU9m52AEw8a4h-BPxGqKdWDkhfhRh56ODW0kppj3YEfq4coXlNqge1nRMHE7TdmwXFlMvyNXjYI7RP79idBeaxL9tsx0BPguroqiJdxsn-pe4_0YVP7RBTUiMVKGqeOfvg-3pwWm0QIRobK-BReLKxu3WpRxtz3X2fkkN6rmz2Hvt_q6wQL7e8AuviYKd48yx5f1kP-QfGZ3GQt8KklRraMtQWlD3RnBFCX-nadQ4D35Q8VIz19bstK0JmpoPX9UA3sgk5KWuw9SN5BF2HTUZNvquCYTllq_wkAV_MT1vG_qlDZmdT8jVMQT0JGFihk1YtK2rALQBXIwCnkAbUOxbuB2QZjJTBTOyb5TFTJV3NRTBpEnq9ww/file [following]\n",
            "--2022-06-01 01:51:44--  https://ucb6bbbf7a5596319cbe64ade88b.dl.dropboxusercontent.com/cd/0/inline2/BmWp_5eOvU6UFOkYKVmeNqKVnQWFLQjVEkAHC0k_IDI7en86QoNDXO9ZccgU9m52AEw8a4h-BPxGqKdWDkhfhRh56ODW0kppj3YEfq4coXlNqge1nRMHE7TdmwXFlMvyNXjYI7RP79idBeaxL9tsx0BPguroqiJdxsn-pe4_0YVP7RBTUiMVKGqeOfvg-3pwWm0QIRobK-BReLKxu3WpRxtz3X2fkkN6rmz2Hvt_q6wQL7e8AuviYKd48yx5f1kP-QfGZ3GQt8KklRraMtQWlD3RnBFCX-nadQ4D35Q8VIz19bstK0JmpoPX9UA3sgk5KWuw9SN5BF2HTUZNvquCYTllq_wkAV_MT1vG_qlDZmdT8jVMQT0JGFihk1YtK2rALQBXIwCnkAbUOxbuB2QZjJTBTOyb5TFTJV3NRTBpEnq9ww/file\n",
            "Reusing existing connection to ucb6bbbf7a5596319cbe64ade88b.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20169156047 (19G) [application/zip]\n",
            "Saving to: ‘data_lmdb_release.zip’\n",
            "\n",
            "data_lmdb_release.z 100%[===================>]  18.78G  21.7MB/s    in 15m 59s \n",
            "\n",
            "2022-06-01 02:07:44 (20.1 MB/s) - ‘data_lmdb_release.zip’ saved [20169156047/20169156047]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ./data_lmdb_release.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTWEtJdQ9ia0",
        "outputId": "4420b51c-8e98-4493-868c-2fcf673443ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./data_lmdb_release.zip\n",
            "   creating: data_lmdb_release/\n",
            "   creating: data_lmdb_release/evaluation/\n",
            "   creating: data_lmdb_release/evaluation/IC03_860/\n",
            "  inflating: data_lmdb_release/evaluation/IC03_860/data.mdb  \n",
            "  inflating: data_lmdb_release/evaluation/IC03_860/lock.mdb  \n",
            "   creating: data_lmdb_release/evaluation/IC03_867/\n",
            "  inflating: data_lmdb_release/evaluation/IC03_867/data.mdb  \n",
            "  inflating: data_lmdb_release/evaluation/IC03_867/lock.mdb  \n",
            "   creating: data_lmdb_release/evaluation/IC13_1015/\n",
            "  inflating: data_lmdb_release/evaluation/IC13_1015/data.mdb  \n",
            "  inflating: data_lmdb_release/evaluation/IC13_1015/lock.mdb  \n",
            "   creating: data_lmdb_release/evaluation/IC13_857/\n",
            "  inflating: data_lmdb_release/evaluation/IC13_857/data.mdb  \n",
            "  inflating: data_lmdb_release/evaluation/IC13_857/lock.mdb  \n",
            "   creating: data_lmdb_release/evaluation/IC15_1811/\n",
            "  inflating: data_lmdb_release/evaluation/IC15_1811/data.mdb  \n",
            "  inflating: data_lmdb_release/evaluation/IC15_1811/lock.mdb  \n",
            "   creating: data_lmdb_release/evaluation/IIIT5k_3000/\n",
            "  inflating: data_lmdb_release/evaluation/IIIT5k_3000/data.mdb  \n",
            "  inflating: data_lmdb_release/evaluation/IIIT5k_3000/lock.mdb  \n",
            "   creating: data_lmdb_release/evaluation/SVT/\n",
            "  inflating: data_lmdb_release/evaluation/SVT/data.mdb  \n",
            "  inflating: data_lmdb_release/evaluation/SVT/lock.mdb  \n",
            "   creating: data_lmdb_release/evaluation/SVTP/\n",
            "  inflating: data_lmdb_release/evaluation/SVTP/data.mdb  \n",
            "  inflating: data_lmdb_release/evaluation/SVTP/lock.mdb  \n",
            "   creating: data_lmdb_release/evaluation/CUTE80/\n",
            "  inflating: data_lmdb_release/evaluation/CUTE80/lock.mdb  \n",
            "  inflating: data_lmdb_release/evaluation/CUTE80/data.mdb  \n",
            "   creating: data_lmdb_release/evaluation/IC15_2077/\n",
            "  inflating: data_lmdb_release/evaluation/IC15_2077/lock.mdb  \n",
            "  inflating: data_lmdb_release/evaluation/IC15_2077/data.mdb  \n",
            "   creating: data_lmdb_release/validation/\n",
            "  inflating: data_lmdb_release/validation/data.mdb  \n",
            "  inflating: data_lmdb_release/validation/lock.mdb  \n",
            "   creating: data_lmdb_release/training/\n",
            "   creating: data_lmdb_release/training/ST/\n",
            "  inflating: data_lmdb_release/training/ST/data.mdb  \n",
            "  inflating: data_lmdb_release/training/ST/lock.mdb  \n",
            "   creating: data_lmdb_release/training/MJ/\n",
            "   creating: data_lmdb_release/training/MJ/MJ_valid/\n",
            "  inflating: data_lmdb_release/training/MJ/MJ_valid/lock.mdb  \n",
            "  inflating: data_lmdb_release/training/MJ/MJ_valid/data.mdb  \n",
            "   creating: data_lmdb_release/training/MJ/MJ_test/\n",
            "  inflating: data_lmdb_release/training/MJ/MJ_test/lock.mdb  \n",
            "  inflating: data_lmdb_release/training/MJ/MJ_test/data.mdb  \n",
            "   creating: data_lmdb_release/training/MJ/MJ_train/\n",
            "  inflating: data_lmdb_release/training/MJ/MJ_train/lock.mdb  \n",
            "  inflating: data_lmdb_release/training/MJ/MJ_train/data.mdb  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integrating `wandb` into training\n",
        "\n",
        "Now that we have the repository with the training code (`deep-text-recognition-benchmark`) and the training dataset downloaded we'll need to edit our `train.py` file to include some Weights and Biases logging functionality.\n",
        "\n",
        "\n",
        "### Edit `train.py`\n",
        "\n",
        "Add the following near the start of your script:\n",
        "\n",
        "```python\n",
        "import wandb\n",
        "wandb.init()\n",
        "```\n",
        "\n",
        "Find where `model.eval()` is called - inside the validation loop, around line 192 (not in the multi-GPU section, unless you are using multiple GPUs to train your model) - and insert `wandb.watch(model, criterion, log=\"all\")`. You will usually want to pass in a logging frequency but due to the way the codebase is written the `wandb` settings will conflict, so we do not set the frequency for now:\n",
        "```python\n",
        "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "```\n",
        "\n",
        "We'll also provide a version of the `train.py` script with these edits for you [here](https://gist.github.com/ap-wb/25737c98a1d52fc36220bffa0248c271).\n",
        "\n",
        "Note that the `num_iter` defaults to 300,000. Since we're working on a toy example and do not want to wait for hours we'll specify a much smaller `num_iter`:"
      ],
      "metadata": {
        "id": "7lPTE2bPB5HY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python3 train.py \\\n",
        "--train_data ./data_lmdb_release/training --valid_data ./data_lmdb_release/validation \\\n",
        "--select_data MJ-ST --batch_ratio 0.5-0.5 --num_iter 100 \\\n",
        "--Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction Attn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u0Wx0jAB1Ds",
        "outputId": "2a435738-7de9-4b37-ed89-dc94060930c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandrea0\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/deep-text-recognition-benchmark/wandb/run-20220601_024601-15x3cmga\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msplendid-hill-27\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/andrea0/deep-text-recognition-benchmark\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/andrea0/deep-text-recognition-benchmark/runs/15x3cmga\u001b[0m\n",
            "Filtering the images containing characters which are not in opt.character\n",
            "Filtering the images whose label is longer than opt.batch_max_length\n",
            "--------------------------------------------------------------------------------\n",
            "dataset_root: ./data_lmdb_release/training\n",
            "opt.select_data: ['MJ', 'ST']\n",
            "opt.batch_ratio: ['0.5', '0.5']\n",
            "--------------------------------------------------------------------------------\n",
            "dataset_root:    ./data_lmdb_release/training\t dataset: MJ\n",
            "sub-directory:\t/MJ/MJ_train\t num samples: 7224586\n",
            "sub-directory:\t/MJ/MJ_valid\t num samples: 802731\n",
            "sub-directory:\t/MJ/MJ_test\t num samples: 891924\n",
            "num total samples of MJ: 8919241 x 1.0 (total_data_usage_ratio) = 8919241\n",
            "num samples of MJ per batch: 192 x 0.5 (batch_ratio) = 96\n",
            "--------------------------------------------------------------------------------\n",
            "dataset_root:    ./data_lmdb_release/training\t dataset: ST\n",
            "sub-directory:\t/ST\t num samples: 5522807\n",
            "num total samples of ST: 5522807 x 1.0 (total_data_usage_ratio) = 5522807\n",
            "num samples of ST per batch: 192 x 0.5 (batch_ratio) = 96\n",
            "--------------------------------------------------------------------------------\n",
            "Total_batch_size: 96+96 = 192\n",
            "--------------------------------------------------------------------------------\n",
            "dataset_root:    ./data_lmdb_release/validation\t dataset: /\n",
            "sub-directory:\t/.\t num samples: 6992\n",
            "--------------------------------------------------------------------------------\n",
            "model input parameters 32 100 20 1 512 256 38 25 TPS ResNet BiLSTM Attn\n",
            "Skip Transformation.LocalizationNetwork.localization_fc2.weight as it is already initialized\n",
            "Skip Transformation.LocalizationNetwork.localization_fc2.bias as it is already initialized\n",
            "Model:\n",
            "DataParallel(\n",
            "  (module): Model(\n",
            "    (Transformation): TPS_SpatialTransformerNetwork(\n",
            "      (LocalizationNetwork): LocalizationNetwork(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "          (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (6): ReLU(inplace=True)\n",
            "          (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "          (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (10): ReLU(inplace=True)\n",
            "          (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "          (12): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (14): ReLU(inplace=True)\n",
            "          (15): AdaptiveAvgPool2d(output_size=1)\n",
            "        )\n",
            "        (localization_fc1): Sequential(\n",
            "          (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "          (1): ReLU(inplace=True)\n",
            "        )\n",
            "        (localization_fc2): Linear(in_features=256, out_features=40, bias=True)\n",
            "      )\n",
            "      (GridGenerator): GridGenerator()\n",
            "    )\n",
            "    (FeatureExtraction): ResNet_FeatureExtractor(\n",
            "      (ConvNet): ResNet(\n",
            "        (conv0_1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn0_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv0_2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn0_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "        (layer1): Sequential(\n",
            "          (0): BasicBlock(\n",
            "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (relu): ReLU(inplace=True)\n",
            "            (downsample): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "        (layer2): Sequential(\n",
            "          (0): BasicBlock(\n",
            "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (relu): ReLU(inplace=True)\n",
            "            (downsample): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (1): BasicBlock(\n",
            "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (relu): ReLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (maxpool3): MaxPool2d(kernel_size=2, stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
            "        (layer3): Sequential(\n",
            "          (0): BasicBlock(\n",
            "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (relu): ReLU(inplace=True)\n",
            "            (downsample): Sequential(\n",
            "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (1): BasicBlock(\n",
            "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (relu): ReLU(inplace=True)\n",
            "          )\n",
            "          (2): BasicBlock(\n",
            "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (relu): ReLU(inplace=True)\n",
            "          )\n",
            "          (3): BasicBlock(\n",
            "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (relu): ReLU(inplace=True)\n",
            "          )\n",
            "          (4): BasicBlock(\n",
            "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (relu): ReLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (layer4): Sequential(\n",
            "          (0): BasicBlock(\n",
            "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (relu): ReLU(inplace=True)\n",
            "          )\n",
            "          (1): BasicBlock(\n",
            "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (relu): ReLU(inplace=True)\n",
            "          )\n",
            "          (2): BasicBlock(\n",
            "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (relu): ReLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (conv4_1): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), bias=False)\n",
            "        (bn4_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv4_2): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
            "        (bn4_2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (AdaptiveAvgPool): AdaptiveAvgPool2d(output_size=(None, 1))\n",
            "    (SequenceModeling): Sequential(\n",
            "      (0): BidirectionalLSTM(\n",
            "        (rnn): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
            "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
            "      )\n",
            "      (1): BidirectionalLSTM(\n",
            "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
            "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (Prediction): Attention(\n",
            "      (attention_cell): AttentionCell(\n",
            "        (i2h): Linear(in_features=256, out_features=256, bias=False)\n",
            "        (h2h): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (score): Linear(in_features=256, out_features=1, bias=False)\n",
            "        (rnn): LSTMCell(294, 256)\n",
            "      )\n",
            "      (generator): Linear(in_features=256, out_features=38, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Trainable params num :  49555182\n",
            "Optimizer:\n",
            "Adadelta (\n",
            "Parameter Group 0\n",
            "    eps: 1e-08\n",
            "    lr: 1\n",
            "    rho: 0.95\n",
            "    weight_decay: 0\n",
            ")\n",
            "------------ Options -------------\n",
            "exp_name: TPS-ResNet-BiLSTM-Attn-Seed1111\n",
            "train_data: ./data_lmdb_release/training\n",
            "valid_data: ./data_lmdb_release/validation\n",
            "manualSeed: 1111\n",
            "workers: 4\n",
            "batch_size: 192\n",
            "num_iter: 100\n",
            "valInterval: 2000\n",
            "saved_model: \n",
            "FT: False\n",
            "adam: False\n",
            "lr: 1\n",
            "beta1: 0.9\n",
            "rho: 0.95\n",
            "eps: 1e-08\n",
            "grad_clip: 5\n",
            "baiduCTC: False\n",
            "select_data: ['MJ', 'ST']\n",
            "batch_ratio: ['0.5', '0.5']\n",
            "total_data_usage_ratio: 1.0\n",
            "batch_max_length: 25\n",
            "imgH: 32\n",
            "imgW: 100\n",
            "rgb: False\n",
            "character: 0123456789abcdefghijklmnopqrstuvwxyz\n",
            "sensitive: False\n",
            "PAD: False\n",
            "data_filtering_off: False\n",
            "Transformation: TPS\n",
            "FeatureExtraction: ResNet\n",
            "SequenceModeling: BiLSTM\n",
            "Prediction: Attn\n",
            "num_fiducial: 20\n",
            "input_channel: 1\n",
            "output_channel: 512\n",
            "hidden_size: 256\n",
            "num_gpu: 1\n",
            "num_class: 38\n",
            "---------------------------------------\n",
            "\n",
            "[1/100] Train loss: 3.60001, Valid loss: 3.67405, Elapsed_time: 2.78328\n",
            "Current_accuracy : 0.000, Current_norm_ED  : 0.01\n",
            "Best_accuracy    : 0.000, Best_norm_ED     : 0.01\n",
            "--------------------------------------------------------------------------------\n",
            "Ground Truth              | Prediction                | Confidence Score & T/F\n",
            "--------------------------------------------------------------------------------\n",
            "30                        | 4222222222222222222222442 | 0.0000\tFalse\n",
            "do                        | a2222rrrrrrrrrrrrfa22rrrr | 0.0000\tFalse\n",
            "staan                     | bbbb2222200ff220fffffffff | 0.0000\tFalse\n",
            "walk                      | j000i00000000000000000000 | 0.0000\tFalse\n",
            "and                       | x0000r00oooooooooorr0rooo | 0.0000\tFalse\n",
            "--------------------------------------------------------------------------------\n",
            "end the training\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msplendid-hill-27\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/andrea0/deep-text-recognition-benchmark/runs/15x3cmga\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220601_024601-15x3cmga/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "You can experiment with logging different parameters. For now, we log the gradients. We'll leave logging the loss curves and accuracy curves as an exercise to the user.\n",
        "\n",
        "\n",
        "*   Gradients from third run, `sandy-glitter-3` here: https://wandb.ai/andrea0/deep-text-recognition-benchmark/runs/2kppjp09?workspace=user-andrea0\n",
        "*   Hardware usage - CPU, GPU, etc. here: https://wandb.ai/andrea0/deep-text-recognition-benchmark?workspace=user-andrea0\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://i.imgur.com/8f68ADy.png\" width=\"80%\" alt=\"Weights & Biases Logo\" />\n"
      ],
      "metadata": {
        "id": "bWAnB3TTH3kH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus: Working with `wandb` Artifacts\n",
        "\n",
        "As you noticed during the process of fine-tuning the EasyOCR model we had to retrieve files from many different locations. It's not ideal to have to download some files from Github which has [bandwidth and storage limits for large files](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-storage-and-bandwidth-usage) and then go to another file-sharing website such as Google Drive or Dropbox for additional files.\n",
        "\n",
        "To log a file (upload a file or set of files) to Weights and Biases Artifact tool simply specify the project name and the name of the file that you want to upload / save as an Artifact.\n",
        "\n",
        "You can optionally pass in parameters such as the Artifact type, e.g., `dataset`, `script`, `model-weights`, etc.\n",
        "\n",
        "```python\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "# Initialize a new W&B run to track this job\n",
        "run = wandb.init(project=\"deep-text-recognition-benchmark\", job_type=\"dataset-creation\")\n",
        "# Create a new artifact (type dataset)\n",
        "dataset = wandb.Artifact('my-dataset', type='dataset')\n",
        "# Add files to the artifact: the train, test, and eval data\n",
        "dataset.add_file('lmdb-dataset.zip')\n",
        "# Log the artifact to save it as an output of this run\n",
        "run.log_artifact(dataset)\n",
        "\n",
        "wandb.finish()\n",
        "```\n",
        "\n",
        "\n",
        "The Artifact will begin syncing and a Weights and Biases link with yellow text will appear in your Colaboratory Notebook. Click on that link, then click on the database icon (looks like a cylindrical can made up of three slices), and then click on your dataset name to be taken to the Overview page in the Artifacts section.\n",
        "\n",
        "Instead of clicking through to reach the Artifacts page you can just navigate to your username and project's Artifact's page by going to the following URL:\n",
        "\n",
        "`https://wandb.ai/andrea0/deep-text-recognition-benchmark/artifacts`\n",
        "\n",
        "`https://wandb.ai/USERNAME/PROJECT_NAME/artifacts`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://i.imgur.com/X5oWFME.png\" width=\"80%\" alt=\"Screenshot of Weights and Biases Artifact tool showing a dataset\" />\n"
      ],
      "metadata": {
        "id": "H4lzIhUTL6Gi"
      }
    }
  ]
}